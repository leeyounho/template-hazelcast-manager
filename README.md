# 실시간 처리를 위한 Hazelcast 기반 인메모리 데이터 그리드

## 1. 소개

이 프로젝트는 기존의 Oracle 데이터베이스 테이블을 고성능 인메모리 데이터 그리드인 **Hazelcast Community Edition 5.2.5**로 대체하기 위한 전략적인 기술 검증(PoC) 및 템플릿입니다.

가장 큰 동기는 처리량이 많은 "Current-State" 데이터 테이블에 대해 디스크 기반 데이터베이스가 가질 수 있는 성능 및 확장성 병목 현상을 극복하는 것입니다. 인메모리 솔루션으로 전환함으로써, 분산 시스템의 운영 요구사항에 맞춰 데이터 집약적인 작업의 지연 시간을 대폭 단축하고, 확장성을 개선하며, 가용성을 향상시키는 것을 목표로 합니다.

이 README 문서는 제공된 소스 코드에 반영된 핵심 아키텍처 결정 사항과 그 배경에 대한 이론적 근거를 설명합니다.

---

## 2. 핵심 아키텍처 결정

관계형 데이터베이스에서 인메모리 그리드로 전환하는 것은 단순한 일대일 매핑이 아닙니다. 기존 애플리케이션의 로직과 운영 요구사항을 존중하면서 Hazelcast의 잠재력을 최대한 활용하기 위해 다음과 같은 결정들이 내려졌습니다.

### 2.1. 왜 Hazelcast Embedded Mode인가?

이 시스템은 Hazelcast를 **Embedded Mode**로 실행하도록 설계되었습니다. 이는 Hazelcast 클러스터 멤버(데이터와 로직)가 애플리케이션 프로세스 내에 함께 위치한다는 것을 의미합니다.

* **근거**:
    * **낮은 지연 시간(Low Latency)**: 이것이 가장 큰 장점입니다. 데이터 그리드를 내장함으로써, 별도의 독립형 캐시 클러스터(Client-Server 모드)와 통신하는 데 필요한 네트워크 홉(hop)을 제거합니다. 데이터 접근은 로컬 함수 호출이 되어 마이크로초 수준의 지연 시간을 기대할 수 있습니다.
    * **단순화된 배포**: 운영 오버헤드를 줄여줍니다. 배포, 관리, 모니터링해야 할 별도의 캐싱 계층이 없습니다. 데이터 그리드의 생명주기는 애플리케이션의 생명주기와 직접적으로 연결되어 기존 배포 스크립트에 완벽하게 들어맞습니다.

### 2.2. 데이터 관리 및 고가용성

엄격한 일관성보다 가용성을 우선시합니다. 애플리케이션 또는 서버 장애 상황에서도 데이터 손실을 방지하도록 구성되었습니다.

* **파티셔닝 및 백업 (`HOST_AWARE`)**:
    * 모든 데이터 항목에 대해 **1개의 동기식 백업**을 구성합니다 (`mapConfig.setBackupCount(1)`).
    * 결정적으로, 파티션 그룹 구성을 `HOST_AWARE`로 설정했습니다.
    * **근거**: 저희 시스템은 물리적 서버 쌍 단위로 확장(scale out)되며, 서버당 여러 애플리케이션 인스턴스가 존재합니다. `HOST_AWARE` 설정은 데이터 파티션과 그 백업이 **절대** 동일한 물리적 머신에 저장되지 않도록 보장합니다. 이는 단일 애플리케이션의 장애뿐만 아니라 서버 전체의 장애에 대한 복원력을 제공합니다.

* **ID 생성 (`FlakeIdGenerator`)**:
    * Oracle의 `DCOL_SEQ` (PK)는 Hazelcast의 `FlakeIdGenerator`로 대체됩니다.
    * **근거**: `FlakeIdGenerator`는 새로운 ID를 생성할 때마다 클러스터 전체의 합의가 필요 없는 분산형 무충돌 ID 생성기입니다. 대략적인 시간 순서를 따르는 64비트 고유 ID를 생성합니다. 이는 대용량 `INSERT` 작업에 이상적이며, 전통적인 데이터베이스 시퀀스의 중앙 집중식 병목 현상을 방지합니다.

### 2.3. 성능 및 메모리 최적화

6GB의 힙을 관리하려면 GC 압박을 최소화하고 성능을 극대화하기 위한 신중한 최적화가 필요합니다.

* **효율적인 직렬화 (`Compact` + `Snappy`)**:
    * 느리고 장황하기로 알려진 표준 자바 직렬화 대신, Hazelcast의 **Compact Serialization**을 사용합니다. 이를 위해 커스텀 `DCOLDataHistSerializer`를 구현했습니다.
    * 또한, 크기가 클 수 있는 `dcolValue` 필드는 직렬화 과정에서 **Snappy** 알고리즘을 사용하여 즉시 압축됩니다.
    * **근거**: 이 이중 접근 방식은 각 `DCOLDataHist` 객체의 메모리 사용량을 획기적으로 줄여줍니다. 객체 크기가 작아지면 힙 사용량이 줄고, 백업을 위한 네트워크 복제 속도가 빨라지며, GC 오버헤드가 감소합니다. 이는 대용량 힙을 사용하는 시스템에서 매우 중요합니다.

* **메모리 포맷 (`BINARY`)**:
    * `dcolHist` 맵의 인메모리 포맷은 `BINARY`로 설정되었습니다.
    * **근거**: 읽기/쓰기 작업이 많은 저희 워크로드에서는 데이터를 직렬화된 `BINARY` 형태로 유지하는 것이 가장 효율적입니다. 이는 모든 `get()` 작업에서 발생하는 역직렬화 CPU 비용을 절약해줍니다. 시스템은 데이터가 실제로 사용될 때만 역직렬화 비용을 지불하며, 이는 효율적인 인덱싱과 결합하여 주요한 성능 향상을 제공하고 GC 압박을 줄입니다.

* **목적 지향적 인덱싱**:
    * 주요 다중 컬럼 Oracle 인덱스는 Hazelcast에서 `eqpId`, `workId`, `controlJobId`, `processJobId` 필드에 대한 **정렬된 복합 인덱스**로 복제되었습니다.
    * **근거**: 이 인덱스는 주요 `SELECT` 및 `DELETE` 쿼리 패턴을 가속화하기 위해 특별히 설계되었습니다. 다른 Oracle 인덱스들은 의도적으로 생략되었는데, 그 기능들이 더 효율적인 Hazelcast 기능으로 대체되었기 때문입니다(예: PK 인덱스는 `IMap` 키로 처리되고, 날짜 기반 인덱스는 TTL 만료로 처리됨).

### 2.4. 데이터 생명주기 및 만료(Eviction)

인메모리 시스템에서 데이터는 영원히 존재할 수 없습니다. 견고한 데이터 생명주기 전략이 필수적입니다.

* **Time-To-Live (TTL)**:
    * `dcolHist` 맵에 3일(`259200`초)의 TTL을 구성했습니다.
    * **근거**: 이는 데이터베이스에서 비용이 많이 들던 주기적인 `DELETE` 작업을 대체합니다. Hazelcast는 데이터 만료를 효율적이고 자동으로 처리합니다. 이는 사용되지 않는 오래된 데이터가 메모리에서 제거되도록 보장하여 힙 사용량을 사전에 제어하는 방법입니다.

* **만료 정책 (Eviction Policy - 안전장치)**:
    * 메모리 고갈에 대한 안전장치로서, `LRU` (Least Recently Used) 만료 정책과 `MaxSizePolicy.PER_NODE`, 그리고 1,000만 개의 항목 크기를 정의했습니다.
    * **근거**: 데이터 유입량이 예기치 않게 급증할 경우, 이 정책은 시스템의 안정성을 보장합니다. 새 데이터를 위한 공간을 확보하기 위해 가장 최근에 사용되지 않은 항목부터 제거하기 시작하여 `OutOfMemoryError` 예외를 방지합니다.

### 2.5. 복원력 (`HazelcastRetryExecutor`)

분산 시스템은 일시적인 장애에 대해 복원력을 갖추어야 합니다.

* **Spring 기반 재시도 메커니즘**:
    * `HazelcastRetryExecutor` 클래스는 `spring-retry`의 `@Retryable`을 사용하여 모든 Hazelcast 작업(`get`, `put`, `delete` 등)을 감쌉니다.
    * `HazelcastException` 발생 시 1초의 간격을 두고 최대 3번까지 재시도하도록 구성되었습니다. 커스텀 `HazelcastRetryListener`는 재시도 상황에 대한 가시성을 위해 로깅을 제공합니다.
    * **근거**: 이는 기존 데이터베이스 접근 로직에서 검증된 복원력 패턴을 그대로 모방합니다. 이를 통해 일시적인 네트워크 문제나 짧은 클러스터 재조정 이벤트가 작업 실패로 이어지지 않도록 하여 애플리케이션의 전반적인 가용성과 견고성을 높입니다.

### 2.6. 트랜잭션 관리 전략

"데이터 일관성보다는 가용성이 중시되지만, 보장할 수 있는 만큼은 최대한 일관성을 보장한다"는 원칙에 따라 트랜잭션 관리 전략을 수립했습니다. Hazelcast Community Edition은 분산 트랜잭션을 지원하지 않으므로, 이를 고려한 설계가 필요했습니다.

* **개별 작업의 원자성(Atomicity)**:
    * `IMap`의 기본 작업(`put`, `get`, `delete` 등)은 개별 키에 대해 원자적으로 동작합니다. 이는 단일 데이터 항목의 `INSERT`, `UPDATE`, `DELETE`는 항상 완전히 성공하거나 실패함을 보장합니다.

* **대량 작업(Bulk Operations)의 한계와 선택**:
    * `saveOrUpdateAll`에서 사용되는 `putAll` 메서드는 여러 항목을 한 번에 전송하여 네트워크 오버헤드를 줄이는 고성능 배치(batch) 작업입니다.
    * **근거**: `putAll`은 단일 원자적 트랜잭션이 아닙니다. 즉, 일부 데이터는 성공하고 일부는 실패할 수 있습니다. 이는 엄격한 일관성이 필요할 경우 단점이 될 수 있지만, 저희 시스템처럼 처리량과 가용성이 더 중요한 시나리오에서는 성능상의 이점을 위해 감수할 수 있는 합리적인 트레이드오프입니다. 재시도 로직을 통해 최종적인 데이터 정합성을 높입니다.

* **EntryProcessor를 이용한 원자적 연산**:
    * `deleteByAttributes`와 같이 조건에 맞는 여러 데이터를 수정/삭제해야 할 때 `EntryProcessor`를 사용합니다.
    * **근거**: `EntryProcessor`는 데이터가 저장된 노드에서 직접 로직을 실행하므로, 불필요한 데이터 이동을 최소화하고 각 엔트리에 대한 연산을 원자적으로 처리할 수 있습니다. 이는 여러 데이터에 대한 작업을 효율적이고 안전하게 수행하는 방법입니다.

---
## 5. 향후 작업 및 고려사항

이 구현은 강력한 기반을 제공하지만, 코드에 `TODO` 주석으로 표시된 바와 같이 향후 개발이 필요한 영역이 있습니다:

* **DB 리포지토리 완성**: `DCOLDataHistDbRepository`는 현재 스텁(stub) 상태입니다. `DCOLDataHistManager`가 전략을 전환할 수 있도록 기존 데이터베이스 로직을 이 클래스로 마이그레이션해야 합니다.
* **동적 구성**: 클러스터 멤버 IP가 현재 하드코딩(`127.0.0.1`)되어 있습니다. 실제 환경에서는 이를 동적으로 탐색하거나 구성 서비스를 통해 관리해야 합니다.
* **성능 튜닝**: 프로덕션과 유사한 환경에서 성능 테스트(`*PerformanceTest.java`)를 실행하여 힙 설정(G1GC vs 현재 CMS), 만료 크기 및 기타 JVM 매개변수를 미세 조정해야 합니다.