# Hazelcast 기반 실시간 데이터 처리 PoC

## 1. 개요

이 프로젝트는 기존 Oracle DB에 의존하던 Current성 테이블을 Hazelcast 기반의 인메모리 데이터 그리드(IMDG)로 전환하기 위한 기술 검증(PoC)입니다.

**목표: 기존 시스템의 DB 의존성 제거입니다. 즉, DB I/O 병목을 제거하고, 실시간 데이터 처리 성능과 시스템 전반의 확장성 및 가용성을 확보하는 것입니다.**

본 문서는 PoC를 진행하며 정의한 운영 및 기술 요구사항을 바탕으로, 아키텍처를 설계하고 주요 기술적 결정을 내린 배경과 그 구현 내용을 공유하기 위해 작성되었습니다.

### 1.1. 시스템 환경 및 제약사항
*  **Java**: OpenJDK 1.8
*  **Hazelcast**: Community Edition 5.2.5 (Embedded Mode)
* **GC & Heap**: G1 GC 및 10GB Heap 사용 (기존 CMS GC, 3GB Heap에서 변경)
* **분산 환경**: 2대의 서버에 총 8개의 애플리케이션이 Active-Active로 동작하며, 서버는 2대 단위로 Scale-out 됩니다.
* **운영 우선순위**: 데이터의 엄격한 일관성보다 시스템의 **가용성**이 더 중요하지만, 보장 가능한 범위 내에서 최대한의 일관성을 확보해야 합니다.

---

## 2. 주요 기술 결정 및 배경

단순히 DB를 Cache로 바꾸는 것이 아니라, 분산 시스템 환경에 맞는 고성능 인메모리 데이터 그리드를 설계하기 위해 다음과 같은 기술적 결정을 내렸습니다.

### 2.1. Hazelcast Embedded Mode 채택

별도의 캐시 클러스터를 구성하는 대신 애플리케이션에 Hazelcast를 내장시키는 Embedded Mode를 선택했습니다.

* **결정 배경**:
    * **성능**: 가장 큰 이유입니다. Client-Server 모드에서 발생하는 네트워크 지연을 원천적으로 제거하여 데이터 접근 속도를 극대화할 수 있습니다.
    * **운영 단순성**: 애플리케이션과 생명주기를 같이하므로 배포 및 운영 관리가 훨씬 단순해집니다. 별도 캐시 클러스터에 대한 배포/모니터링 포인트를 줄일 수 있습니다.

### 2.2. 데이터 관리 및 가용성 확보 전략

장애 상황에서도 데이터를 유실하지 않고 서비스를 유지하는 것을 최우선으로 고려했습니다.

* **HOST_AWARE 백업 설정**:
    * 데이터 백업본은 1개로 설정하고(`backupCount(1)`), 파티션 그룹은 `HOST_AWARE`로 구성했습니다.
    * **결정 배경**: TC 시스템은 서버 2대 단위로 확장됩니다. `HOST_AWARE` 설정은 원본 데이터와 백업 데이터가 절대 같은 물리 장비에 위치하지 않도록 보장해줍니다. 특정 애플리케이션 하나가 죽는 상황을 넘어, 물리 서버 한 대가 통째로 다운되는 최악의 경우에도 데이터 유실을 막기 위한 결정입니다.

* **분산 ID 생성 (`FlakeIdGenerator`)**:
    * DB 시퀀스를 대체하기 위해 Hazelcast의 `FlakeIdGenerator`를 도입했습니다.
    * **결정 배경**: DB 시퀀스는 모든 애플리케이션이 하나의 리소스에 접근해야 하므로 병목이 될 수 있습니다. `FlakeIdGenerator`는 각 노드에서 충돌 없이 고유 ID를 생성해주므로, 대량의 데이터가 동시 다발적으로 발생하는 환경에서 `INSERT` 성능을 보장하는 데 유리합니다.

### 2.3. 성능 및 메모리 최적화

3GB → 10GB로 힙 메모리를 키우고 GC 부담을 줄이는 데 집중했습니다.

* **커스텀 직렬화 (Compact + Snappy)**:
    * 무거운 자바 기본 직렬화 대신, Hazelcast의 Compact Serialization 포맷을 사용하도록 커스텀 Serializer를 구현했습니다.
    * 특히 사이즈가 클 수 있는 `dcolValue` 필드는 Snappy 알고리즘으로 압축해서 저장하도록 처리했습니다.
    * **결정 배경**: 객체 직렬화 방식은 메모리 사용량과 네트워크 트래픽에 직접적인 영향을 줍니다. 이 두 가지 최적화를 통해 객체 사이즈를 줄여 힙 메모리를 아끼고, 백업 복제 시 네트워크 부하를 줄여 GC 발생 가능성을 낮췄습니다.

* **BINARY 포맷 저장**:
    * `IMap`의 인메모리 포맷은 `BINARY`로 지정했습니다.
    * **결정 배경**: 데이터를 `OBJECT` 형태로 저장하면 `get` 할 때마다 역직렬화가 필요 없어 편하지만, TC 시스템은 `get`보다 `put`이 훨씬 빈번하고, `get`한 데이터를 바로 사용하는 경우가 많습니다. `BINARY` 포맷은 역직렬화 비용을 실제 데이터가 필요한 시점까지 미뤄 CPU 부담을 줄이고, 결과적으로 GC 튜닝에 더 유리합니다.

* **핵심 조회 조건에 대한 인덱싱**:
    * 가장 빈번한 조회 조건인 `eqpId`, `workId`, `controlJobId`, `processJobId` 4개 필드에 대해서만 복합 인덱스를 생성했습니다.
    * **결정 배경**: 인덱스는 조회를 빠르게 하지만 `put` 성능을 저하시키고 메모리를 더 사용합니다. 모든 조회 케이스를 커버하기보다, 가장 핵심적인 조회 성능을 보장하는 선에서 트레이드오프했습니다. PK 조회는 `IMap`의 Key 조회가 대체하고, 기간 기반 삭제는 TTL 기능으로 대체하므로 불필요한 인덱스는 과감히 제거했습니다.

### 2.4. 데이터 생명주기 관리

인메모리 특성상 데이터는 무한정 쌓아둘 수 없으므로, 명확한 삭제 정책이 필요합니다.

* **TTL (Time-To-Live) 기반 데이터 삭제**:
    * 모든 데이터에 3일의 TTL(`259200`초)을 설정했습니다.
    * **결정 배경**: DB에서처럼 스케줄러로 대량의 `DELETE` 쿼리를 실행하는 것은 인메모리 시스템에 큰 부담입니다. TTL을 이용하면 Hazelcast가 알아서 만료된 데이터를 효율적으로 처리해주므로, 힙 사용량을 안정적으로 관리할 수 있습니다.

* **LRU 기반의 데이터 만료(Eviction) 정책**:
    * TTL 외에, 노드별 최대 데이터 개수(1,000만 건)를 초과하면 가장 오래 사용되지 않은 데이터(LRU)를 삭제하는 정책을 추가했습니다.
    * **결정 배경**: 일시적인 데이터 폭증으로 힙 메모리가 가득 차서 시스템 전체가 죽는 상황을 막기 위한 '최후의 안전장치'입니다.

### 2.5. 장애 대응 및 복원력

분산 환경에서는 일시적인 네트워크 단절이나 노드 장애가 언제든 발생할 수 있습니다.

* **Spring-Retry 기반 재시도 로직**:
    * 기존 DB 접근 시 사용하던 `spring-retry` 로직을 그대로 Hazelcast 연산에 적용했습니다. `HazelcastException`이 발생하면 설정된 횟수만큼 재시도합니다.
    * **결정 배경**: 클러스터 재조정 등으로 인해 발생하는 일시적인 오류 때문에 작업 전체가 실패하는 것을 막기 위함입니다. 이미 검증된 재시도 패턴을 재사용하여 시스템 안정성을 높였습니다.

### 2.6. 트랜잭션 관리 전략

운영 요구사항인 '가용성 우선, 단 최대한의 일관성 보장'을 충족시키는 데 초점을 맞췄습니다. Hazelcast Community Edition은 분산 트랜잭션을 지원하지 않으므로, 이를 감안한 현실적인 전략을 선택했습니다.

* **개별 작업의 원자성(Atomicity) 활용**:
    * `IMap`의 `put`, `get`, `delete` 같은 기본 작업은 Key 하나에 대해 원자성을 보장합니다. 단일 데이터의 CRUD는 항상 안전하게 처리됩니다.

* **대량 작업(Bulk Operations) 선택과 트레이드오프**:
    * 대량 저장 시에는 `putAll`을 사용합니다. 이는 모든 데이터가 하나의 트랜잭션으로 묶이지는 않지만, 여러 `put`을 호출하는 것보다 훨씬 효율적입니다.
    * **결정 배경**: 엄격한 데이터 일관성보다 높은 처리량이 더 중요한 시스템이므로, 성능을 위해 `putAll`을 선택했습니다. 실패 케이스는 재시도 로직으로 보완하여 최종적인 일관성을 추구합니다.

* **EntryProcessor를 이용한 원자적 연산**:
    * 조건부 대량 삭제 시에는 `EntryProcessor`를 활용합니다.
    * **결정 배경**: `EntryProcessor`는 데이터가 실제 저장된 노드에서 연산을 수행하므로 네트워크 트래픽을 최소화하고, 각 데이터에 대한 수정/삭제 작업을 원자적으로 처리할 수 있어 안전합니다.
